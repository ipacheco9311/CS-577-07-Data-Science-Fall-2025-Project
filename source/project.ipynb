{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set Up for Project Imports\n",
   "id": "73acc9f141075611"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from data.api import UcIrvineAPI, UcIrvineDatasetIDs\n",
    "import data.wrangling_utils\n",
    "import duckdb\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    median_absolute_error,\n",
    ")"
   ],
   "id": "33a7e5543a6aea8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "source_directory = Path.cwd()\n",
    "ROOT = source_directory.parent\n",
    "data_directory  = ROOT / 'data'\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))"
   ],
   "id": "26b64af0f12ae2ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Auto-reload code changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "9b8f4360f956b974",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pandas.set_option('display.max_colwidth', None)  # show all text in cells\n",
    "# pandas.set_option(\"display.max_rows\", 100_000)\n",
    "pandas.options.mode.copy_on_write = True\n",
    "pandas.set_option('display.float_format', lambda x: '%.2f' % x)"
   ],
   "id": "8c220ffa268b6e71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UcIrvine Data",
   "id": "418a7487f08fecdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "uci = UcIrvineAPI.fetch_dataset(repo_id=UcIrvineDatasetIDs.Apartment_For_Rent_Classified.value)\n",
    "uci_df: pandas.DataFrame = uci.data.original.reset_index()"
   ],
   "id": "e211d935a855b77d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "uci_df.describe()",
   "id": "288c8e49a1c0de1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "uci_df.shape",
   "id": "7d04d75b33378b1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Cleaning",
   "id": "73c73a3f02fe5d92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "clean_uci_df: pandas.DataFrame = data.wrangling_utils.clean(uci_df)",
   "id": "4b7f4fdbe8b5a317",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clean_uci_df: pandas.DataFrame = data.wrangling_utils.cast(clean_uci_df)\n",
    "clean_uci_df.info()"
   ],
   "id": "cbe9ca40b74e3389",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "clean_uci_df.rename(columns={'id':'uci_id'}, inplace=True)",
   "id": "5e178e8294f53933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'id' not in clean_uci_df.columns:\n",
    "    clean_uci_df.insert(0, 'id', np.arange(len(clean_uci_df)))"
   ],
   "id": "8b5d4c57b5972404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SQL",
   "id": "908afd0f2acf3fb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    _ = connection.execute(\n",
    "    '''\n",
    "        CREATE TABLE IF NOT EXISTS listings (\n",
    "            id            BIGINT,\n",
    "            uci_id        BIGINT,\n",
    "            category      VARCHAR,\n",
    "            title         VARCHAR,\n",
    "            body          VARCHAR,\n",
    "            amenities     VARCHAR,\n",
    "            bathrooms     BIGINT,\n",
    "            bedrooms      BIGINT,\n",
    "            currency      VARCHAR,\n",
    "            fee           BOOLEAN,\n",
    "            has_photo     BOOLEAN,\n",
    "            pets_allowed  VARCHAR,\n",
    "            price         DOUBLE,\n",
    "            price_display DOUBLE,\n",
    "            price_type    VARCHAR,\n",
    "            square_feet   DOUBLE,\n",
    "            address       VARCHAR,\n",
    "            cityname      VARCHAR,\n",
    "            state         VARCHAR,\n",
    "            latitude      DOUBLE,\n",
    "            longitude     DOUBLE,\n",
    "            source        VARCHAR,\n",
    "            time          DOUBLE\n",
    "        );\n",
    "        '''\n",
    "    )\n",
    "    _ = connection.register(\"df_view\", clean_uci_df)\n",
    "    rows_before = connection.execute(\"SELECT COUNT(*) FROM listings\").fetchone()[0]\n",
    "    _ = connection.execute(\"INSERT INTO listings SELECT * FROM df_view WHERE id NOT IN (SELECT id FROM listings)\")\n",
    "    rows_after = connection.execute(\"SELECT COUNT(*) FROM listings\").fetchone()[0]\n",
    "    print(f'[Info] Inserted {rows_after - rows_before} records into listings.')\n"
   ],
   "id": "2fbb3f3ede22333b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory Data Analysis\n",
   "id": "f0b1d1aa6ca33dcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = None\n",
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT category,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL\n",
    "    GROUP BY category\n",
    "    ORDER BY category;\n",
    "    ''').fetchdf()\n",
    "df"
   ],
   "id": "eea0427e3f10ba61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT price_type,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    price_type IS NOT NULL\n",
    "    GROUP BY price_type\n",
    "    ORDER BY price_type;\n",
    "    ''').fetchdf()\n",
    "df"
   ],
   "id": "764cf02a4ecc75dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT currency,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    currency IS NOT NULL\n",
    "    GROUP BY currency\n",
    "    ORDER BY currency;\n",
    "    ''').fetchdf()\n",
    "df"
   ],
   "id": "e4c2b69de2082374",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Top 5 Listing Sources",
   "id": "c012c83f6e197b10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT source,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    source IS NOT NULL\n",
    "    GROUP BY source\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5;\n",
    "    ''').fetchdf()\n",
    "df"
   ],
   "id": "cba51ea910cd40b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Distribution of Count of Listings per State",
   "id": "7e8eff0b9ed14456"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT *,\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    state IS NOT NULL AND category IS NOT NULL AND\n",
    "    price_type  IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly'\n",
    "    ORDER BY state;\n",
    "    ''').fetchdf()"
   ],
   "id": "e83c3523ec1d08cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ax = sns.countplot(data=df, x='state', width=0.5)\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()"
   ],
   "id": "ff8cbaf4536132e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There is class imbalance among the states. Each state is not equally represented in the data set. But this is a natural imbalances so we won't resample. Some states have larger than others (e.g., california than vermont).",
   "id": "99bb6355b6d66988"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Descriptive Statistics of all listings",
   "id": "dd4ecc47f7772a4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "741ac628560f9cf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An apartment with 9 bedrooms, a monthly cost of $120,000, and 50,000 square feet? Is this actually an apartment? Let's examine the listings by combinations of bedrooms and bathrooms, and distributions of prices and square footage.. It will give insight on the type of properties in our data.",
   "id": "b4d6c2237296cfe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Removing Noise",
   "id": "61021ba267762473"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT bedrooms, bathrooms,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL AND price_type  IS NOT NULL AND\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly'\n",
    "    GROUP BY bedrooms, bathrooms\n",
    "    ORDER BY bedrooms, bathrooms;\n",
    "    ''').fetchdf()\n",
    "df"
   ],
   "id": "14a40bb8ef71c1ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It appears studios, townhomes, houses, mansions, or penthouses are also in the data. Some properties don't even have a bedroom? Other properties have 4+ bedrooms and bathrooms? That doesn't make sense for a typical apartment so we will limit our listings to 1-3 bedrooms and 1-3 bathrooms.",
   "id": "2596d914efd2dea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute(f'''\n",
    "    SELECT bedrooms, bathrooms, price\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL AND price_type  IS NOT NULL AND\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly' AND\n",
    "    bedrooms >= 1 AND bedrooms <= 3 AND\n",
    "    bathrooms >= 1 AND bathrooms <= 3;\n",
    "    ''').fetchdf()\n",
    "df.describe()"
   ],
   "id": "2923d62a8e4a9f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Why is 3 bed and 3 bath priced at $33,0000? That seems overpriced even for a luxury apartment. Let's examine the distribution of listing prices for our data.",
   "id": "72013236f870a367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT price,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL AND price_type  IS NOT NULL AND\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND price IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly' AND\n",
    "    bedrooms >= 1 AND bedrooms <= 3 AND\n",
    "    bathrooms >= 1 AND bathrooms <= 3\n",
    "    GROUP BY price;\n",
    "    ''').fetchdf()"
   ],
   "id": "32545e401af27ed0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(df['price'], bins=50)\n",
    "plt.xlim(df['price'].min(), df['price'].max())\n",
    "plt.show()"
   ],
   "id": "f3c938ff1603b054",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The distribution of prices is unimodal, and it is strongly right skewed where most the data is centered around 2,000-3,0000 USD. There are extreme outliers of 10,000-30,000. Maybe luxurious apartments/penthouse are causing this.",
   "id": "7cfe5d2992703ff2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT bedrooms, bathrooms, price\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL AND price_type  IS NOT NULL AND\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND price IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly' AND\n",
    "    bedrooms >= 1 AND bedrooms <= 3 AND\n",
    "    bathrooms >= 1 AND bathrooms <= 3 AND\n",
    "    price >= 500 AND price <= 10000;\n",
    "    ''').fetchdf()\n",
    "df.describe()"
   ],
   "id": "5c40683f83841ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's examine the distribution of prices again.",
   "id": "e4692ed0a7b61230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT price,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL AND price_type  IS NOT NULL AND\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND price IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly' AND\n",
    "    bedrooms >= 1 AND bedrooms <= 3 AND\n",
    "    bathrooms >= 1 AND bathrooms <= 3 AND\n",
    "    price >= 500 AND price <= 10000\n",
    "    GROUP BY price;\n",
    "    ''').fetchdf()\n",
    "sns.histplot(df['price'], bins=50)\n",
    "plt.show()"
   ],
   "id": "f453ad2ceb23632b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The distribution of the listing prices is still right skewed but we got rid of some properties that weren't representative of a typical apartment in terms of prrice. Now let's examine the distribution of the square footage of the listings since ~5,000 square foot apartment still seems large for a max 3 bedroom and 3 bathroom apartment.",
   "id": "6fbaa6e43ff76dd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT square_feet,\n",
    "    COUNT(*) AS count\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL AND price_type  IS NOT NULL AND\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND price IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly' AND\n",
    "    bedrooms >= 1 AND bedrooms <= 3 AND\n",
    "    bathrooms >= 1 AND bathrooms <= 3 AND\n",
    "    price >= 500 AND price <= 10000\n",
    "    GROUP BY square_feet\n",
    "    ''').fetchdf()\n",
    "sns.histplot(df['square_feet'], bins=24)\n",
    "plt.xlim(df['square_feet'].min(), df['square_feet'].max())\n",
    "plt.show()"
   ],
   "id": "218ba00a310b212b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT bathrooms, bedrooms, price, square_feet\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    category IS NOT NULL AND price_type  IS NOT NULL AND\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND price IS NOT NULL AND\n",
    "    category LIKE '%apartment%' AND price_type='monthly' AND\n",
    "    bedrooms >= 1 AND bedrooms <= 3 AND\n",
    "    bathrooms >= 1 AND bathrooms <= 3 AND\n",
    "    price >= 500 AND price <= 10000 AND\n",
    "    square_feet <= 4000;\n",
    "    ''').fetchdf()\n",
    "df.describe()"
   ],
   "id": "f83e69f9d0771b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(df['square_feet'], bins=24)\n",
    "plt.xlim(df['square_feet'].min(), df['square_feet'].max())\n",
    "plt.show()"
   ],
   "id": "7668bc9e55e7266e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with duckdb.connect(database= (data_directory / 'apartments.duckdb')) as connection:\n",
    "    df = connection.execute('''\n",
    "    SELECT bathrooms, bedrooms, price, square_feet, state, latitude, longitude, cityname\n",
    "    FROM listings\n",
    "    WHERE\n",
    "    bedrooms IS NOT NULL AND bathrooms IS NOT NULL AND\n",
    "    price IS NOT NULL AND square_feet IS NOT NULL AND\n",
    "    price_type IS NOT NULL AND state IS NOT NULL AND\n",
    "    latitude IS NOT NULL AND longitude IS NOT NULL AND\n",
    "    cityname IS NOT NULL AND category is NOT NULL AND\n",
    "    bedrooms >= 1 AND bedrooms <= 3 AND\n",
    "    bathrooms >= 1 AND bathrooms <= 3 AND\n",
    "    price >= 500 AND price <= 10000 AND\n",
    "    square_feet <= 4000 AND\n",
    "    price_type = 'monthly' AND\n",
    "    category LIKE '%apartment%';\n",
    "    ''').fetchdf()"
   ],
   "id": "54ef79b52bc579b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "2cab1b54f8a6d4ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "85a4d578d9d8aa02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "percent_change = (1.0 - (df.shape[0] / uci_df.shape[0])) * 100\n",
    "print(f\"Percent change {percent_change:.3f}%\")"
   ],
   "id": "2d1d2f1684026264",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## More Skewness Analysis",
   "id": "3b5a6c420e108688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[['price', 'square_feet']].skew()",
   "id": "71038b66f31fd914",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Feature       | Skew     | Interpretation            | Action               |\n",
    "| ------------- |----------|---------------------------|----------------------|\n",
    "| `price`       | **2.28** | **Strongly right-skewed** | Should transform     |\n",
    "| `square_feet` | **1.17** | **Strongly right-skewed** | Should transform     |\n"
   ],
   "id": "e04f0f10c2608711"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Most the data is small to large apartments prices around $1,500 - 3,000 but a handful of extremely underpriced/overpriced apartments are skewing the data. So we will perform a log transformation to reduce their influence.",
   "id": "936c2ca86e720f77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['price_log'] = np.log1p(df['price'])\n",
    "df['square_feet_log'] = np.log1p(df['square_feet'])\n",
    "df[['price_log', 'square_feet_log']].skew()"
   ],
   "id": "191a5d41952eb94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Feature       | Skew              | Interpretation         |\n",
    "|---------------|-------------------|-------------------------|\n",
    "| `price`       | **2.28 -> 0.36**  | **Weakly right-skewed** |\n",
    "| `square_feet` | **1.17 -> -0.21** | **Weakly left-skewed**  |"
   ],
   "id": "9d6e02e88a799e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(df['price_log'], bins=24)\n",
    "plt.xlim(df['price_log'].min(), df['price_log'].max())\n",
    "plt.show()"
   ],
   "id": "83ad1804f245ad5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(df['square_feet_log'], bins=24)\n",
    "plt.xlim(df['square_feet_log'].min(), df['square_feet_log'].max())\n",
    "plt.show()"
   ],
   "id": "b3b78d44e4427cff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grid = sns.pairplot(df[['bathrooms', 'bedrooms', 'price_log', 'square_feet_log']], kind='scatter', corner=True)",
   "id": "42b409fecca15de4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There only seems a weak linear relationship between log_price and square_feet_log.",
   "id": "d2c575001e42800b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "correlation = df[['bathrooms', 'bedrooms', 'price_log', 'square_feet_log']].corr(numeric_only=True)",
   "id": "b5f2a9a636491b4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "correlation",
   "id": "1ef8c4c5f4a39c05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Correlation Heatmap\", fontsize=14)\n",
    "sns.heatmap(correlation, annot=True, linewidths=0.5, cmap='mako')\n",
    "plt.show()"
   ],
   "id": "bf12fee9ceee5c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Variable Types\n",
    "\n",
    "- Discrete / ordinal: bedrooms, bathrooms — integer counts\n",
    "- Continuous: price_log, square_feet_log — continuous and normalized\n",
    "- We used Pearson correlation, the relationships involving discrete counts are approximate linear associations, not strict parametric correlations — but they’re still informative here since the discrete values are ordered and range reasonably (0–9).\n",
    "\n",
    "\n",
    "| Pair                            | Correlation | Interpretation                                                                                                                                            |\n",
    "| ------------------------------- |-------------| --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **bathrooms ↔ bedrooms**        | **0.65**    | Strong positive relationship — homes with more bedrooms usually have more bathrooms.                                                                      |\n",
    "| **bathrooms ↔ square_feet_log** | **0.68**    | Strong positive correlation — larger houses naturally have more bathrooms.                                                                                |\n",
    "| **bedrooms ↔ square_feet_log**  | **0.66**    | Same strong pattern — larger homes have more bedrooms.                                                                                                    |\n",
    "| **price_log ↔ square_feet_log** | **0.36**    | Moderate positive relationship — price generally rises with size, but not perfectly (other factors matter).                                               |\n",
    "| **price_log ↔ bathrooms**       | **0.31**    | Mild correlation — price increases somewhat with bathroom count, but not linearly.                                                                        |\n",
    "| **price_log ↔ bedrooms**        | **0.21**    | Weak correlation — price doesn’t increase as predictably with bedroom count, possibly because extra bedrooms add less marginal value than square footage. |\n"
   ],
   "id": "11858c9eda38c9b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Skewnesss by State",
   "id": "3578f5aa2af6ebfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state_groups = df.groupby(\"state\")\n",
    "for state, group in state_groups:\n",
    "    numeric_df = group[[\"bathrooms\", \"bedrooms\", \"price_log\", \"square_feet_log\"]]\n",
    "    skew_per_state = numeric_df.skew()\n",
    "    print(f\"{state}\\n{skew_per_state}\\n\")"
   ],
   "id": "580bd40a3e55a43e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correlation by State",
   "id": "79c6418525d05a40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correlation_by_state = {}\n",
    "for state, group in state_groups:\n",
    "    numeric_df = group[[\"bathrooms\", \"bedrooms\", \"price_log\", \"square_feet_log\"]]\n",
    "    correlation_by_state[state] = numeric_df.corr()"
   ],
   "id": "fa294c3d5604945c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_states = len(correlation_by_state.keys())\n",
    "# Grid size (fits 51)\n",
    "rows = 8\n",
    "cols = 7\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "axes = axes.flatten()\n",
    "for idx, state in enumerate(correlation_by_state.keys()):\n",
    "    ax = axes[idx]\n",
    "    corr = correlation_by_state[state]\n",
    "\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        ax=ax,\n",
    "        cmap=\"mako\",\n",
    "        square=True,\n",
    "        xticklabels=True,\n",
    "        yticklabels=True,\n",
    "        annot=True,\n",
    "        cbar=False\n",
    "    )\n",
    "\n",
    "    ax.set_title(state, fontsize=14)\n",
    "    ax.tick_params(axis='x', labelrotation=45, labelsize=10)\n",
    "    ax.tick_params(axis='y', labelrotation=45, labelsize=10)\n",
    "\n",
    "# Hide any unused subplot cells\n",
    "for j in range(len(correlation_by_state.keys()), len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Correlation Matrices by State\", fontsize=18)\n",
    "plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.99))\n",
    "plt.show()"
   ],
   "id": "7544e94844df4460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# K Means Clustering",
   "id": "47fc083057d5c1ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clusterer = KMeans(n_clusters=30, random_state=42)\n",
    "df['geo_cluster'] = clusterer.fit_predict(df[['latitude', 'longitude']])"
   ],
   "id": "b8f8f2a0fa49a179",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    hue=\"geo_cluster\",\n",
    "    palette=\"tab20\",\n",
    "    s=30\n",
    ")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    title=\"Cluster\",\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(1,1),\n",
    "    loc=\"upper left\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ],
   "id": "da7a338625a96f07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df[['square_feet_log', 'bedrooms', 'bathrooms', 'state', 'geo_cluster', 'cityname']]\n",
    "y = df['price_log']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"square_feet_log_poly\", PolynomialFeatures(degree=2, include_bias=False), ['square_feet_log']),\n",
    "    ('structura_linear', 'passthrough', ['bedrooms', 'bathrooms']),\n",
    "    ('city_encoded', OneHotEncoder(handle_unknown='ignore'), ['cityname']),\n",
    "    ('neighborhood_cluster_encoded', OneHotEncoder(handle_unknown='ignore'), ['geo_cluster'])\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('linreg', LinearRegression()),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# ==========================\n",
    "# 7. Metrics helper (includes Adjusted R²)\n",
    "# ==========================\n",
    "def adjusted_r2(y_true, y_pred, n_features: int) -> float:\n",
    "    \"\"\"Compute Adjusted R² given predictions and number of original features.\"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    n = len(y_true)\n",
    "    p = n_features\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "def print_regression_metrics(split_name, y_true, y_pred, n_features: int):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adj_r2 = adjusted_r2(y_true, y_pred, n_features)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    med_ae = median_absolute_error(y_true, y_pred)\n",
    "\n",
    "    print(f\"=== {split_name} Metrics ===\")\n",
    "    print(f\"R²:              {r2:.2f}\")\n",
    "    print(f\"Adjusted R²:     {adj_r2:.2f}\")\n",
    "    print(f\"MSE:             {mse:.2f}\")\n",
    "    print(f\"RMSE:            {rmse:.2f}\")\n",
    "    print(f\"MAE:             {mae:.2f}\")\n",
    "    print(f\"MAPE:            {mape:.2f}\")\n",
    "    print(f\"Median Abs Err:  {med_ae:.2f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 8. Interpret real-world error from log-space metrics\n",
    "# ==========================\n",
    "def interpret_log_error_metrics(split_name, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Interpret log-space error metrics in real price terms.\n",
    "    Assumes y_true and y_pred are log(price).\n",
    "    \"\"\"\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae_log = mean_absolute_error(y_true, y_pred)\n",
    "    med_ae_log = median_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Convert log errors to multiplicative factors\n",
    "    rmse_factor = np.exp(rmse_log)\n",
    "    mae_factor = np.exp(mae_log)\n",
    "    med_factor = np.exp(med_ae_log)\n",
    "\n",
    "    rmse_pct = (rmse_factor - 1.0) * 100.0\n",
    "    mae_pct = (mae_factor - 1.0) * 100.0\n",
    "    med_pct = (med_factor - 1.0) * 100.0\n",
    "\n",
    "    print(f\"=== {split_name} Real-World Error Interpretation ===\")\n",
    "    print(f\"Typical error (RMSE):    x {rmse_factor:.3f}  (predictions are typically off by ~{rmse_pct:.1f}% in true price)\")\n",
    "    print(f\"Average error (MAE):     x {mae_factor:.3f}  (On average, predictions are off by ~{mae_pct:.1f}% in true price)\")\n",
    "    print(f\"Typical core error (Median AE): x {med_factor:.3f}  (For half the homes, predictions are off within ~{med_pct:.1f}% in true price)\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# number of ORIGINAL features (bathrooms, bedrooms, sqft_log, state)\n",
    "n_features = model.named_steps[\"preprocess\"].transform(X_train).shape[1]\n",
    "\n",
    "print_regression_metrics(\"Train\", y_train, y_train_pred, n_features)\n",
    "print_regression_metrics(\"Test\", y_test, y_test_pred, n_features)\n",
    "\n",
    "interpret_log_error_metrics(\"Train\", y_train, y_train_pred)\n",
    "interpret_log_error_metrics(\"Test\", y_test, y_test_pred)\n"
   ],
   "id": "dab6a042b29d7c2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Metric               | Train Value | Test Value | Interpretation                                                                                                                   |\n",
    "|----------------------|-------------|------------|----------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **R²**               | 0.78        | 0.76       | % of variance explained. Model captures ~78% of training variation and ~76% of unseen variation **strong fit, low overfitting**. |\n",
    "| **MSE**              | 0.04        | 0.04       | Average squared error. Test MSE only slightly higher **good generalization**.                                                    |\n",
    "| **RMSE**             | 0.20        | 0.20       | Typical prediction error magnitude, ~20% deviation in normalized/log scale.                                                      |\n",
    "| **MAE**              | 0.15        | 0.15       | Average absolute error, predictions off by ~15%.                                                                                 |\n",
    "| **MAPE**             | 0.02        | 0.02       | Mean Absolute Percentage Error ~2% if target is scaled, or ~2% error in/log space. Very low **consistent accuracy**.             |\n",
    "| **Median Abs Error** | 0.11        | 0.12       | Half of predictions are within **11-12% error**. Indicates tight, stable core accuracy.                                          |\n"
   ],
   "id": "d4432e0b51859a6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_resid = y_train - y_train_pred\n",
    "test_resid  = y_test - y_test_pred"
   ],
   "id": "6d517fdc6c7741cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build DataFrames for easy plotting\n",
    "train_df = pandas.DataFrame({\n",
    "    \"Fitted\": y_train_pred,\n",
    "    \"Residual\": train_resid,\n",
    "    \"Split\": \"Train\"\n",
    "})\n",
    "\n",
    "test_df = pandas.DataFrame({\n",
    "    \"Fitted\": y_test_pred,\n",
    "    \"Residual\": test_resid,\n",
    "    \"Split\": \"Test\"\n",
    "})\n",
    "\n",
    "resid_df = pandas.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    data=resid_df,\n",
    "    x=\"Fitted\",\n",
    "    y=\"Residual\",\n",
    "    hue=\"Split\",\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.axhline(0, linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "plt.title(\"Residuals vs Fitted\")\n",
    "plt.xlabel(\"Fitted\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "417cd7e5eaa65dd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DO NOT DELETE MIGHT NEED\n",
    "# import json\n",
    "# s = cleaned_uci_df['bathrooms'].explode()\n",
    "# global_counts = s.value_counts().to_dict()\n",
    "# global_counts\n",
    "# print(f'BAD_DATA: {json.dumps(BAD_DATA['cityname'], indent=2)}')  # CHANGE COL\n",
    "# uci_df[\"state_full\"] = uci_df[\"state\"].str.upper().map(STATE_MAP)\n",
    "# print(uci_df.shape)\n",
    "# uci_df.dropna(subset=[\"state_full\"], inplace=True)\n",
    "# uci_df.shape"
   ],
   "id": "b8d3584b5226cf12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Linear Regression",
   "id": "dce5a7d01034fcaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lin_regression_residuals = {}\n",
    "for state, group in state_groups:\n",
    "    # choose numeric columns only\n",
    "    X = group[[\"bathrooms\", \"bedrooms\", \"square_feet_log\"]]\n",
    "    y = group[['price_log']]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    # Predictions and residuals\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    lin_regression_residuals[state] = y_pred, residuals"
   ],
   "id": "7da1a64b7c5c96d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Grid size (fits 51)\n",
    "rows = 8\n",
    "cols = 7\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 6))\n",
    "axes = axes.flatten()\n",
    "for idx, state in enumerate(lin_regression_residuals.keys()):\n",
    "    ax = axes[idx]\n",
    "    y_pred, residuals = lin_regression_residuals[state]\n",
    "    ax.scatter(y_pred, residuals)\n",
    "    ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_title(state, fontsize=14)\n",
    "    ax.set_xlabel('Predicted price_log', fontsize=10)\n",
    "    ax.set_ylabel('Residuals', fontsize=10)\n",
    "    ax.set_title(state, fontsize=14)\n",
    "# Hide any unused subplot cells\n",
    "for j in range(len(lin_regression_residuals.keys()), len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "plt.tight_layout(rect=(0.0, 0.0, 1.0, 0.99))\n",
    "plt.show()"
   ],
   "id": "2c6be007861f7ecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Classification Model",
   "id": "9f31468fe7f23587",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Goal: classifiy the listings as overpriced or fair",
   "id": "1c8bfd6f8b7976b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_transformed = X_train.copy()\n",
    "X_train_transformed['pred_price_log'] = y_train_pred\n",
    "X_train_transformed['pct_overpriced'] = (y_train - X_train_transformed['pred_price_log']) / X_train_transformed['pred_price_log']\n",
    "X_train_transformed['overpriced'] = (X_train_transformed['pct_overpriced'] > 0.20).astype(int)\n",
    "\n",
    "numeric_features = ['bedrooms', 'bathrooms', 'square_feet_log', 'pred_price_log', 'pct_overpriced']\n",
    "categorical_features = ['geo_cluster', 'cityname']\n",
    "\n",
    "preprocessor_clf = ColumnTransformer([\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "clf_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor_clf),\n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ],
   "id": "7971c205fe66bdd1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
